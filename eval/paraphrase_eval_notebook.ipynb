{"cells":[{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":17832,"status":"ok","timestamp":1652337062302,"user":{"displayName":"Daniil Arushanov","userId":"08628604466675582560"},"user_tz":360},"id":"rV7Y56tqpy93","outputId":"3dbe8793-8bb3-4cc9-f44f-15bc2ec52b26"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')\n","path = \"/content/drive/My Drive/Senior Design/\""]},{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":8869,"status":"ok","timestamp":1652337071167,"user":{"displayName":"Daniil Arushanov","userId":"08628604466675582560"},"user_tz":360},"id":"TutSvauVrykP","outputId":"1c1ab37e-969e-4de5-a9dc-06421903ec05"},"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting transformers\n","  Downloading transformers-4.18.0-py3-none-any.whl (4.0 MB)\n","\u001b[K     |████████████████████████████████| 4.0 MB 14.3 MB/s \n","\u001b[?25hCollecting pyyaml>=5.1\n","  Downloading PyYAML-6.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (596 kB)\n","\u001b[K     |████████████████████████████████| 596 kB 56.6 MB/s \n","\u001b[?25hRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.3)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.64.0)\n","Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.11.3)\n","Collecting sacremoses\n","  Downloading sacremoses-0.0.53.tar.gz (880 kB)\n","\u001b[K     |████████████████████████████████| 880 kB 64.5 MB/s \n","\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n","Collecting huggingface-hub<1.0,>=0.1.0\n","  Downloading huggingface_hub-0.6.0-py3-none-any.whl (84 kB)\n","\u001b[K     |████████████████████████████████| 84 kB 3.1 MB/s \n","\u001b[?25hCollecting tokenizers!=0.11.3,<0.13,>=0.11.1\n","  Downloading tokenizers-0.12.1-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (6.6 MB)\n","\u001b[K     |████████████████████████████████| 6.6 MB 85.3 MB/s \n","\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.6.0)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.21.6)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (4.2.0)\n","Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (3.0.8)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.8.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2021.10.8)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n","Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n","Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.1.0)\n","Building wheels for collected packages: sacremoses\n","  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for sacremoses: filename=sacremoses-0.0.53-py3-none-any.whl size=895260 sha256=c1dcf7b4ecf73dec064984657892e4e54cfb4ab9fa2190d7651b7af3df8a02c5\n","  Stored in directory: /root/.cache/pip/wheels/87/39/dd/a83eeef36d0bf98e7a4d1933a4ad2d660295a40613079bafc9\n","Successfully built sacremoses\n","Installing collected packages: pyyaml, tokenizers, sacremoses, huggingface-hub, transformers\n","  Attempting uninstall: pyyaml\n","    Found existing installation: PyYAML 3.13\n","    Uninstalling PyYAML-3.13:\n","      Successfully uninstalled PyYAML-3.13\n","Successfully installed huggingface-hub-0.6.0 pyyaml-6.0 sacremoses-0.0.53 tokenizers-0.12.1 transformers-4.18.0\n"]}],"source":["!pip install transformers"]},{"cell_type":"code","execution_count":4,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":46094,"status":"ok","timestamp":1652337117253,"user":{"displayName":"Daniil Arushanov","userId":"08628604466675582560"},"user_tz":360},"id":"frlGlwgzpnPw","outputId":"579acab5-8007-4d1c-be02-b90e6ccc0761"},"outputs":[{"output_type":"stream","name":"stderr","text":["Some weights of the model checkpoint at /content/drive/My Drive/Senior Design/models/roberta-large-finetuned-qqp were not used when initializing RobertaForSequenceClassification: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n","- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"]}],"source":["import csv\n","import json\n","from tqdm import tqdm\n","from transformers import AutoTokenizer, AutoModelForSequenceClassification\n","import torch\n","import random\n","from sklearn.model_selection import train_test_split\n","\n","model_path = path + \"models/roberta-large-finetuned-qqp\"\n","\n","model = AutoModelForSequenceClassification.from_pretrained(model_path)\n","model.cuda()\n","tokenizer = AutoTokenizer.from_pretrained(model_path)"]},{"cell_type":"code","execution_count":5,"metadata":{"executionInfo":{"elapsed":6848,"status":"ok","timestamp":1652337124093,"user":{"displayName":"Daniil Arushanov","userId":"08628604466675582560"},"user_tz":360},"id":"q_N29XADvsLN"},"outputs":[],"source":["data_path = path + \"data/datascience_qa.json\"\n","\n","qa_data = {}\n","\n","with open(data_path, \"r\", encoding='utf-8') as f:\n","    qa_data = json.load(f)\n","\n","questions = []\n","\n","for i in qa_data:\n","  question = qa_data[i][\"question:\"]\n","  answer = qa_data[i][\"answer:\"]\n","  \n","  query= str(question).strip()\n","  answer = str(answer).strip()\n","\n","  if answer != \"nan\":\n","    # len(tokenizer(query)['input_ids'])\n","    questions.append(query)"]},{"cell_type":"code","execution_count":6,"metadata":{"executionInfo":{"elapsed":3,"status":"ok","timestamp":1652337124093,"user":{"displayName":"Daniil Arushanov","userId":"08628604466675582560"},"user_tz":360},"id":"Zu-pK6iGvu4D"},"outputs":[],"source":["# Split dataset into training and validation set.\n","train_ques, valid_ques = train_test_split(\n","    questions, train_size=0.99, shuffle=True, random_state=47)"]},{"cell_type":"code","source":["len(train_ques)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"w_fTnZ9I9mEP","executionInfo":{"status":"ok","timestamp":1652337124714,"user_tz":360,"elapsed":2,"user":{"displayName":"Daniil Arushanov","userId":"08628604466675582560"}},"outputId":"69e3d130-8569-4259-9101-87140311ecf4"},"execution_count":7,"outputs":[{"output_type":"execute_result","data":{"text/plain":["13597"]},"metadata":{},"execution_count":7}]},{"cell_type":"code","execution_count":6,"metadata":{"colab":{"base_uri":"https://localhost:8080/","output_embedded_package_id":"1mS59oDvFxHWXozg2KupttKlTKCbxmaIb"},"id":"OJvqS6covbXy","outputId":"b8225d4f-35c8-42cf-95cd-85a734986ab0","executionInfo":{"status":"ok","timestamp":1652326877075,"user_tz":360,"elapsed":5959194,"user":{"displayName":"Daniil Arushanov","userId":"08628604466675582560"}}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Output hidden; open in https://colab.research.google.com to view."},"metadata":{}}],"source":["all_preds = []\n","no_overlap = []\n","overlap = 0\n","for i, ques in tqdm(enumerate(valid_ques)):\n","    print(f\"Question: {i+1}/{len(valid_ques)}\")\n","    for i in range(0, len(train_ques), 3):\n","      if i + 2 >= len(train_ques):\n","        break\n","      batch = [(ques, x) for x in train_ques[i:i+3]]\n","      inputs = tokenizer.batch_encode_plus(\n","          batch,\n","          add_special_tokens=True,\n","          truncation=True,\n","          padding=True,\n","          return_tensors=\"pt\",\n","      )\n","      inputs[\"input_ids\"] = inputs[\"input_ids\"].cuda()\n","      inputs[\"attention_mask\"] = inputs[\"attention_mask\"].cuda()\n","      logits = model(**inputs)[0]\n","      preds = torch.argmax(logits, dim=1).tolist()\n","      probs = torch.softmax(logits, dim=1).tolist()\n","      all_preds.append(preds)\n","      if max(preds) > 0.5:\n","        print(batch)\n","        overlap += 1\n","\n","    print(\"Matching questions = {:d}\".format(overlap))\n","print(\"Matching questions = {:d}\".format(overlap))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"F5i2msWH1esF"},"outputs":[],"source":[""]}],"metadata":{"accelerator":"GPU","colab":{"background_execution":"on","collapsed_sections":[],"machine_shape":"hm","name":"paraphrase.ipynb","provenance":[],"authorship_tag":"ABX9TyPLhKDGnxV/K8dKNAHyCTTr"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}